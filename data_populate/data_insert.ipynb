{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c74fe5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard imports and config\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "from io import StringIO\n",
    "from kiteconnect import KiteConnect\n",
    "from sqlalchemy import create_engine, text\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# load .env if present\n",
    "\n",
    "load_dotenv()  \n",
    "\n",
    "# DB connection string: prefer env var for security\n",
    "DB_URI = os.getenv(\"RAILWAY_DB_URI\") or \"postgresql://postgres:ZpfLDFFOLJemAIEkOTBpEjCuBWYyIwSm@switchback.proxy.rlwy.net:19114/railway\"\n",
    "\n",
    "# Kite credentials - you must set these as env vars or replace here (not recommended to hardcode)\n",
    "KITE_API_KEY = os.getenv(\"KITE_API_KEY\")  # e.g. \"your_api_key\"\n",
    "KITE_API_SECRET = os.getenv(\"KITE_API_SECRET\")\n",
    "KITE_ACCESS_TOKEN = os.getenv(\"KITE_ACCESS_TOKEN\")  # if you have an access token already\n",
    "\n",
    "# create sqlalchemy engine (connection pooling + convenience)\n",
    "\n",
    "engine = create_engine(DB_URI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1d405967",
   "metadata": {},
   "outputs": [],
   "source": [
    "kite = KiteConnect(api_key=KITE_API_KEY)\n",
    "kite.set_access_token(access_token=KITE_ACCESS_TOKEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8618dace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables and indexes created/ensured.\n"
     ]
    }
   ],
   "source": [
    "# create tables if not exists using raw SQL\n",
    "create_sql = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS instruments (\n",
    "  instrument_token BIGINT PRIMARY KEY,\n",
    "  tradingsymbol TEXT NOT NULL,\n",
    "  name TEXT,\n",
    "  exchange TEXT,\n",
    "  segment TEXT,\n",
    "  instrument_type TEXT,\n",
    "  lot_size INTEGER,\n",
    "  tick_size NUMERIC(18,8),\n",
    "  is_active BOOLEAN DEFAULT TRUE,\n",
    "  metadata JSONB,\n",
    "  created_at TIMESTAMP WITH TIME ZONE DEFAULT now(),\n",
    "  updated_at TIMESTAMP WITH TIME ZONE DEFAULT now()\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS ohlcv (\n",
    "  id BIGSERIAL PRIMARY KEY,\n",
    "  instrument_token BIGINT NOT NULL REFERENCES instruments(instrument_token),\n",
    "  tradingsymbol TEXT NOT NULL,\n",
    "  exchange TEXT NOT NULL,\n",
    "  interval TEXT NOT NULL,\n",
    "  ts TIMESTAMP WITH TIME ZONE NOT NULL,\n",
    "  open NUMERIC(18,6) NOT NULL,\n",
    "  high NUMERIC(18,6) NOT NULL,\n",
    "  low NUMERIC(18,6) NOT NULL,\n",
    "  close NUMERIC(18,6) NOT NULL,\n",
    "  volume BIGINT NOT NULL,\n",
    "  oi BIGINT NULL,\n",
    "  raw JSONB NULL,\n",
    "  created_at TIMESTAMP WITH TIME ZONE DEFAULT now()\n",
    ");\n",
    "\n",
    "-- unique constraint to avoid duplicate candles\n",
    "DO $$\n",
    "BEGIN\n",
    "  IF NOT EXISTS (\n",
    "    SELECT 1 FROM pg_indexes WHERE schemaname = 'public' AND indexname = 'ux_ohlcv_inst_int_ts'\n",
    "  ) THEN\n",
    "    CREATE UNIQUE INDEX ux_ohlcv_inst_int_ts ON ohlcv(instrument_token, interval, ts);\n",
    "  END IF;\n",
    "END$$;\n",
    "\n",
    "-- index for fast range queries\n",
    "CREATE INDEX IF NOT EXISTS idx_ohlcv_inst_ts ON ohlcv(instrument_token, ts DESC);\n",
    "CREATE INDEX IF NOT EXISTS idx_ohlcv_symbol_ts ON ohlcv(tradingsymbol, ts DESC);\n",
    "\"\"\"\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(create_sql))\n",
    "    conn.commit()\n",
    "\n",
    "print(\"Tables and indexes created/ensured.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a2bd4092",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_historical(instrument_token, from_date, to_date, interval='5minute', max_chunk_days=None):\n",
    "    \"\"\"\n",
    "    Fetch historical data from Kite Connect API.\n",
    "    Returns a DataFrame with columns: ts, open, high, low, close, volume, oi\n",
    "    \"\"\"\n",
    "    # Map our interval names to Kite's expected format\n",
    "    interval_map = {\n",
    "        '1minute': 'minute',\n",
    "        '3minute': '3minute',\n",
    "        '5minute': '5minute',\n",
    "        '15minute': '15minute',\n",
    "        '30minute': '30minute',\n",
    "        '60minute': '60minute',\n",
    "        '180minute': '60minute',  # Kite doesn't support 180min, use 60min\n",
    "        '1day': 'day',\n",
    "        '1week': 'week',\n",
    "        '1month': 'month',\n",
    "        '1year': 'day'  # Kite doesn't support year, use day and aggregate later\n",
    "    }\n",
    "    \n",
    "    kite_interval = interval_map.get(interval, interval)\n",
    "    \n",
    "    try:\n",
    "        # Kite API expects dates in specific format\n",
    "        from_dt = from_date if isinstance(from_date, datetime) else pd.to_datetime(from_date)\n",
    "        to_dt = to_date if isinstance(to_date, datetime) else pd.to_datetime(to_date)\n",
    "        \n",
    "        # Add small delay to avoid rate limiting\n",
    "        time.sleep(0.25)\n",
    "        \n",
    "        # Fetch data from Kite\n",
    "        records = kite.historical_data(\n",
    "            instrument_token=instrument_token,\n",
    "            from_date=from_dt,\n",
    "            to_date=to_dt,\n",
    "            interval=kite_interval,\n",
    "            continuous=False,\n",
    "            oi=True\n",
    "        )\n",
    "        \n",
    "        if not records:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(records)\n",
    "        \n",
    "        # Rename 'date' column to 'ts' if it exists\n",
    "        if 'date' in df.columns:\n",
    "            df = df.rename(columns={'date': 'ts'})\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        err_str = str(e)\n",
    "        # If 503 or rate limit, raise to trigger retry with backoff\n",
    "        if '503' in err_str or 'rate' in err_str.lower() or 'too many' in err_str.lower():\n",
    "            print(f\"Rate limit/503 error: {e}\")\n",
    "            raise  # Let retry logic handle it\n",
    "        print(f\"Error fetching historical data: {e}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4d4bedf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalize_sym(s):\n",
    "    \"\"\"normalize ticker strings for matching (uppercase, remove punctuation/spaces)\"\"\"\n",
    "    s = str(s).upper()\n",
    "    s = re.sub(r'[\\s\\.\\&\\/\\(\\)\\,\\'\\\"]', '', s)\n",
    "    s = s.replace('-', '')\n",
    "    return s\n",
    "\n",
    "def df_to_postgres_copy(df, table_name, conn, columns=None, commit=True):\n",
    "    \"\"\"\n",
    "    fast bulk insert using COPY FROM STDIN\n",
    "    df: pandas DataFrame must have columns matching the target table order\n",
    "    conn: psycopg2 connection\n",
    "    columns: list of column names to specify in COPY command (optional)\n",
    "    commit: whether to commit after COPY (default True)\n",
    "    \"\"\"\n",
    "    buf = StringIO()\n",
    "    df.to_csv(buf, index=False, header=False, na_rep='\\\\N')  # nulls as \\N for COPY\n",
    "    buf.seek(0)\n",
    "    cur = conn.cursor()\n",
    "    try:\n",
    "        # If columns specified, use them in COPY command\n",
    "        if columns:\n",
    "            cols_str = ', '.join(columns)\n",
    "            copy_sql = f\"COPY {table_name} ({cols_str}) FROM STDIN WITH (FORMAT CSV)\"\n",
    "        else:\n",
    "            copy_sql = f\"COPY {table_name} FROM STDIN WITH (FORMAT CSV)\"\n",
    "        cur.copy_expert(copy_sql, buf)\n",
    "        if commit:\n",
    "            conn.commit()\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        raise\n",
    "    finally:\n",
    "        cur.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a8496760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruments upserted: 498\n"
     ]
    }
   ],
   "source": [
    "# path to your matched CSV (from earlier step)\n",
    "matched_csv = \"./data/nifty500_instruments_matched.csv\"\n",
    "\n",
    "# read CSV\n",
    "df_inst = pd.read_csv(matched_csv, dtype=str)  # read as strings to be safe\n",
    "df_inst = df_inst.fillna('')  # replace NaNs with empty string for fields\n",
    "\n",
    "# convert numeric-ish columns if they exist\n",
    "if 'instrument_token' in df_inst.columns:\n",
    "    df_inst['instrument_token'] = df_inst['instrument_token'].astype(int)\n",
    "if 'lot_size' in df_inst.columns:\n",
    "    df_inst['lot_size'] = pd.to_numeric(df_inst['lot_size'], errors='coerce').fillna(1).astype(int)\n",
    "if 'tick_size' in df_inst.columns:\n",
    "    df_inst['tick_size'] = pd.to_numeric(df_inst['tick_size'], errors='coerce').fillna(0.0)\n",
    "\n",
    "# prepare rows and upsert into instruments table\n",
    "rows = []\n",
    "for _, r in df_inst.iterrows():\n",
    "    rows.append((\n",
    "        int(r.get('instrument_token', 0)),\n",
    "        r.get('tradingsymbol', ''),\n",
    "        r.get('name', ''),\n",
    "        r.get('exchange', ''),\n",
    "        r.get('segment', ''),\n",
    "        r.get('instrument_type', ''),\n",
    "        int(r.get('lot_size', 1)) if r.get('lot_size','')!='' else 1,\n",
    "        float(r.get('tick_size', 0.0)) if r.get('tick_size','')!='' else 0.0,\n",
    "        True,\n",
    "        None\n",
    "    ))\n",
    "\n",
    "upsert_sql = \"\"\"\n",
    "INSERT INTO instruments (\n",
    "  instrument_token, tradingsymbol, name, exchange, segment, instrument_type, lot_size, tick_size, is_active, metadata\n",
    ") VALUES %s\n",
    "ON CONFLICT (instrument_token) DO UPDATE\n",
    "SET\n",
    "  tradingsymbol = EXCLUDED.tradingsymbol,\n",
    "  name = EXCLUDED.name,\n",
    "  exchange = EXCLUDED.exchange,\n",
    "  segment = EXCLUDED.segment,\n",
    "  instrument_type = EXCLUDED.instrument_type,\n",
    "  lot_size = EXCLUDED.lot_size,\n",
    "  tick_size = EXCLUDED.tick_size,\n",
    "  is_active = EXCLUDED.is_active,\n",
    "  metadata = EXCLUDED.metadata,\n",
    "  updated_at = now()\n",
    "\"\"\"\n",
    "\n",
    "conn = psycopg2.connect(DB_URI)\n",
    "try:\n",
    "    cur = conn.cursor()\n",
    "    execute_values(cur, upsert_sql, rows, template=None, page_size=500)\n",
    "    conn.commit()\n",
    "    print(\"Instruments upserted:\", len(rows))\n",
    "finally:\n",
    "    cur.close()\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a567a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c9dcc7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-define ingest_candles_df (uses df_to_postgres_copy from earlier cells)\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "def ingest_candles_df(df_candles, instrument_token, tradingsymbol, exchange='NSE', interval='5minute'):\n",
    "    \"\"\"\n",
    "    Normalize df_candles and bulk insert into ohlcv table using COPY to temp table + INSERT ON CONFLICT.\n",
    "    Returns number of inserted rows.\n",
    "    \"\"\"\n",
    "    df = df_candles.copy()\n",
    "    if df.empty:\n",
    "        print(\"no rows to ingest\")\n",
    "        return 0\n",
    "\n",
    "    # attach metadata columns expected by DB\n",
    "    df['tradingsymbol'] = tradingsymbol\n",
    "    df['instrument_token'] = int(instrument_token)\n",
    "    df['exchange'] = exchange\n",
    "    df['interval'] = interval\n",
    "\n",
    "    # ensure ts is timezone-aware UTC\n",
    "    df['ts'] = pd.to_datetime(df['ts'])\n",
    "    if df['ts'].dt.tz is None:\n",
    "        df['ts'] = df['ts'].dt.tz_localize('UTC')\n",
    "\n",
    "    # ensure numeric types\n",
    "    for col in ['open','high','low','close']:\n",
    "        df[col] = df[col].astype(float)\n",
    "\n",
    "    df['volume'] = df.get('volume', pd.Series([0]*len(df))).fillna(0).astype(int)\n",
    "    if 'oi' not in df.columns:\n",
    "        df['oi'] = None\n",
    "\n",
    "    # reorder to match COPY target (excluding auto-generated id and raw columns)\n",
    "    df_to_copy = df[['instrument_token','tradingsymbol','exchange','interval','ts','open','high','low','close','volume','oi']]\n",
    "\n",
    "    conn = psycopg2.connect(DB_URI)\n",
    "    cur = conn.cursor()\n",
    "    try:\n",
    "        # Create temp table with same structure\n",
    "        cur.execute(\"\"\"\n",
    "            CREATE TEMP TABLE temp_ohlcv (\n",
    "                instrument_token BIGINT,\n",
    "                tradingsymbol TEXT,\n",
    "                exchange TEXT,\n",
    "                interval TEXT,\n",
    "                ts TIMESTAMP WITH TIME ZONE,\n",
    "                open NUMERIC(18,6),\n",
    "                high NUMERIC(18,6),\n",
    "                low NUMERIC(18,6),\n",
    "                close NUMERIC(18,6),\n",
    "                volume BIGINT,\n",
    "                oi BIGINT\n",
    "            ) ON COMMIT DROP\n",
    "        \"\"\")\n",
    "        \n",
    "        # COPY to temp table (no conflicts possible) - don't commit yet to keep temp table\n",
    "        columns = ['instrument_token','tradingsymbol','exchange','interval','ts','open','high','low','close','volume','oi']\n",
    "        df_to_postgres_copy(df_to_copy, 'temp_ohlcv', conn, columns=columns, commit=False)\n",
    "        \n",
    "        # Insert from temp to main table with ON CONFLICT DO NOTHING\n",
    "        cur.execute(\"\"\"\n",
    "            INSERT INTO ohlcv (instrument_token, tradingsymbol, exchange, interval, ts, open, high, low, close, volume, oi)\n",
    "            SELECT instrument_token, tradingsymbol, exchange, interval, ts, open, high, low, close, volume, oi\n",
    "            FROM temp_ohlcv\n",
    "            ON CONFLICT (instrument_token, interval, ts) DO NOTHING\n",
    "        \"\"\")\n",
    "        \n",
    "        inserted = cur.rowcount\n",
    "        conn.commit()\n",
    "        \n",
    "        if inserted > 0:\n",
    "            print(f\"Inserted {inserted} new rows for {tradingsymbol} ({instrument_token}) interval={interval}\")\n",
    "        else:\n",
    "            print(f\"No new rows for {tradingsymbol} ({instrument_token}) interval={interval} (all duplicates)\")\n",
    "        return inserted\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        raise\n",
    "    finally:\n",
    "        cur.close()\n",
    "        conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2477263d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intervals to fetch: ['1minute', '3minute', '5minute', '15minute', '30minute', '60minute', '180minute', '1day', '1week', '1month', '1year']\n"
     ]
    }
   ],
   "source": [
    "# intervals you asked for\n",
    "INTERVALS = [\n",
    "    '1minute', '3minute', '5minute', '15minute', '30minute',\n",
    "    '60minute', '180minute', '1day', '1week', '1month', '1year'\n",
    "]\n",
    "\n",
    "# map some alternate names Kite may accept: '1hour' -> '60minute', '3hour' -> '180minute'\n",
    "# if Kite doesn't accept e.g. '1year' you can skip; the fetch function will just return empty for unsupported intervals.\n",
    "# chunk size (days) to request per API call per interval â€” conservative defaults\n",
    "CHUNK_DAYS_BY_INTERVAL = {\n",
    "    '1minute': 7,     # small chunks for 1m\n",
    "    '3minute': 10,\n",
    "    '5minute': 14,\n",
    "    '15minute': 30,\n",
    "    '30minute': 45,\n",
    "    '60minute': 90,\n",
    "    '180minute': 180,\n",
    "    '1day': 365,      # 1 year per chunk is okay for daily\n",
    "    '1week': 365*3,   # weeks: bigger chunk\n",
    "    '1month': 365*5,\n",
    "    '1year': 365*20,  # asking for multi-year might be accepted as single rows; keep large chunk\n",
    "}\n",
    "\n",
    "# canonicalize interval names used by our fetch/ingest functions (if you used '1hour' earlier replace with 60minute)\n",
    "# ensure CHUNK_DAYS_BY_INTERVAL contains each INTERVAL\n",
    "for itv in INTERVALS:\n",
    "    if itv not in CHUNK_DAYS_BY_INTERVAL:\n",
    "        CHUNK_DAYS_BY_INTERVAL[itv] = 90\n",
    "\n",
    "print(\"Intervals to fetch:\", INTERVALS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "54b9f50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ingest_jobs table ensured.\n"
     ]
    }
   ],
   "source": [
    "# create an ingest_jobs table to track progress per instrument_token + interval\n",
    "create_jobs_sql = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS ingest_jobs (\n",
    "  job_id BIGSERIAL PRIMARY KEY,\n",
    "  instrument_token BIGINT NOT NULL,\n",
    "  tradingsymbol TEXT NOT NULL,\n",
    "  interval TEXT NOT NULL,\n",
    "  start_ts TIMESTAMP WITH TIME ZONE NOT NULL,\n",
    "  end_ts TIMESTAMP WITH TIME ZONE NOT NULL,\n",
    "  last_ingested_ts TIMESTAMP WITH TIME ZONE,\n",
    "  status TEXT NOT NULL DEFAULT 'pending',   -- pending, running, done, error\n",
    "  last_error TEXT,\n",
    "  updated_at TIMESTAMP WITH TIME ZONE DEFAULT now()\n",
    ");\n",
    "\n",
    "CREATE UNIQUE INDEX IF NOT EXISTS ux_ingest_job_inst_int ON ingest_jobs(instrument_token, interval, start_ts, end_ts);\n",
    "\"\"\"\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(create_jobs_sql))\n",
    "    conn.commit()\n",
    "\n",
    "print(\"ingest_jobs table ensured.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "aec8f60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "def make_date_chunks(start_dt, end_dt, chunk_days):\n",
    "    \"\"\"\n",
    "    Yield (chunk_start, chunk_end) datetimes from start_dt to end_dt inclusive,\n",
    "    each chunk with length chunk_days.\n",
    "    \"\"\"\n",
    "    cur = start_dt\n",
    "    while cur < end_dt:\n",
    "        nxt = min(cur + timedelta(days=chunk_days), end_dt)\n",
    "        yield (cur, nxt)\n",
    "        cur = nxt + timedelta(seconds=1)  # avoid overlapping by 1s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3ea4cd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "from sqlalchemy import text as sql_text\n",
    "\n",
    "def process_instrument_interval(instrument_token, tradingsymbol, interval, start_ts, end_ts,\n",
    "                                chunk_days=None, max_retries=3, sleep_between_chunks=0.5):\n",
    "    \"\"\"\n",
    "    Processes one instrument + one interval across time range [start_ts, end_ts].\n",
    "    - Uses ingest_jobs to record progress and allow resume.\n",
    "    - chunk_days: override; otherwise uses CHUNK_DAYS_BY_INTERVAL.\n",
    "    Returns: dict with summary: {'inserted': int, 'chunks': int, 'status': 'done'/'error'}\n",
    "    \"\"\"\n",
    "    chunk_days = chunk_days or CHUNK_DAYS_BY_INTERVAL.get(interval, 90)\n",
    "    inserted_total = 0\n",
    "    chunks = 0\n",
    "\n",
    "    # upsert a job row (or fetch existing)\n",
    "    with engine.begin() as conn:\n",
    "        existing = conn.execute(sql_text(\"\"\"\n",
    "            SELECT job_id, last_ingested_ts, status FROM ingest_jobs\n",
    "            WHERE instrument_token = :it AND interval = :itv AND start_ts = :s AND end_ts = :e\n",
    "            FOR UPDATE\n",
    "        \"\"\"), {\"it\": instrument_token, \"itv\": interval, \"s\": start_ts, \"e\": end_ts}).fetchone()\n",
    "\n",
    "        if existing is None:\n",
    "            r = conn.execute(sql_text(\"\"\"\n",
    "                INSERT INTO ingest_jobs (instrument_token, tradingsymbol, interval, start_ts, end_ts, status)\n",
    "                VALUES (:it, :sym, :itv, :s, :e, :status)\n",
    "                RETURNING job_id\n",
    "            \"\"\"), {\"it\": instrument_token, \"sym\": tradingsymbol, \"itv\": interval, \"s\": start_ts, \"e\": end_ts, \"status\": \"pending\"})\n",
    "            job_id = r.scalar()\n",
    "            last_ingested_ts = None\n",
    "            status = 'pending'\n",
    "        else:\n",
    "            job_id, last_ingested_ts, status = existing\n",
    "            job_id = int(job_id)\n",
    "\n",
    "    # if job is marked done skip\n",
    "    if status == 'done':\n",
    "        print(f\"[skip] job {instrument_token} {tradingsymbol} {interval} already done.\")\n",
    "        return {\"inserted\": 0, \"chunks\": 0, \"status\": \"done\"}\n",
    "\n",
    "    # determine resume start: if last_ingested_ts exists, resume after that\n",
    "    resume_after = last_ingested_ts if last_ingested_ts is not None else start_ts\n",
    "\n",
    "    try:\n",
    "        # mark running\n",
    "        with engine.begin() as conn:\n",
    "            conn.execute(sql_text(\"UPDATE ingest_jobs SET status='running', updated_at=now(), last_error=NULL WHERE job_id=:jid\"),\n",
    "                         {\"jid\": job_id})\n",
    "\n",
    "        for chunk_start, chunk_end in make_date_chunks(resume_after, end_ts, chunk_days):\n",
    "            chunks += 1\n",
    "            success = False\n",
    "            attempts = 0\n",
    "\n",
    "            # retry loop per chunk\n",
    "            while not success and attempts < max_retries:\n",
    "                attempts += 1\n",
    "                try:\n",
    "                    print(f\"Fetching {tradingsymbol} ({instrument_token}) interval={interval} chunk {chunk_start} -> {chunk_end} (attempt {attempts})\")\n",
    "                    df_chunk = fetch_historical(instrument_token, chunk_start, chunk_end, interval=interval, max_chunk_days=chunk_days)\n",
    "                    if df_chunk.empty:\n",
    "                        print(\"  chunk returned empty.\")\n",
    "                    else:\n",
    "                        # ingest\n",
    "                        inserted = ingest_candles_df(df_chunk, instrument_token, tradingsymbol, exchange='NSE', interval=interval)\n",
    "                        inserted_total += inserted\n",
    "                    # update last_ingested_ts to chunk_end\n",
    "                    with engine.begin() as conn:\n",
    "                        conn.execute(sql_text(\"\"\"\n",
    "                            UPDATE ingest_jobs SET last_ingested_ts = :ts, updated_at = now()\n",
    "                            WHERE job_id = :jid\n",
    "                        \"\"\"), {\"ts\": chunk_end, \"jid\": job_id})\n",
    "                    success = True\n",
    "                except Exception as e:\n",
    "                    print(f\"  error on attempt {attempts} for chunk: {e}\")\n",
    "                    traceback.print_exc()\n",
    "                    time.sleep(1.0 * attempts)  # exponential-ish backoff\n",
    "            # short polite pause between chunks\n",
    "            time.sleep(sleep_between_chunks)\n",
    "\n",
    "        # finished all chunks: mark done\n",
    "        with engine.begin() as conn:\n",
    "            conn.execute(sql_text(\"UPDATE ingest_jobs SET status='done', updated_at=now() WHERE job_id=:jid\"), {\"jid\": job_id})\n",
    "        print(f\"[done] {tradingsymbol} {interval}: inserted_total={inserted_total}, chunks={chunks}\")\n",
    "        return {\"inserted\": inserted_total, \"chunks\": chunks, \"status\": \"done\"}\n",
    "    except Exception as e:\n",
    "        err = str(e)[:2000]\n",
    "        with engine.begin() as conn:\n",
    "            conn.execute(sql_text(\"UPDATE ingest_jobs SET status='error', last_error=:err, updated_at=now() WHERE job_id=:jid\"),\n",
    "                         {\"err\": err, \"jid\": job_id})\n",
    "        print(f\"[error] job failed: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return {\"inserted\": inserted_total, \"chunks\": chunks, \"status\": \"error\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c464856a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found instruments: 10\n",
      "=== Processing instrument: 360ONE 3343617\n",
      "  1minute: fetching 1 years of data\n",
      "Fetching 360ONE (3343617) interval=1minute chunk 2024-11-26 12:32:28.772354+00:00 -> 2024-12-03 12:32:28.772354+00:00 (attempt 1)\n",
      "No new rows for 360ONE (3343617) interval=1minute (all duplicates)\n",
      "Fetching 360ONE (3343617) interval=1minute chunk 2024-12-03 12:32:29.772354+00:00 -> 2024-12-10 12:32:29.772354+00:00 (attempt 1)\n",
      "No new rows for 360ONE (3343617) interval=1minute (all duplicates)\n",
      "Fetching 360ONE (3343617) interval=1minute chunk 2024-12-10 12:32:30.772354+00:00 -> 2024-12-17 12:32:30.772354+00:00 (attempt 1)\n",
      "No new rows for 360ONE (3343617) interval=1minute (all duplicates)\n",
      "Fetching 360ONE (3343617) interval=1minute chunk 2024-12-17 12:32:31.772354+00:00 -> 2024-12-24 12:32:31.772354+00:00 (attempt 1)\n",
      "No new rows for 360ONE (3343617) interval=1minute (all duplicates)\n",
      "Fetching 360ONE (3343617) interval=1minute chunk 2024-12-24 12:32:32.772354+00:00 -> 2024-12-31 12:32:32.772354+00:00 (attempt 1)\n",
      "No new rows for 360ONE (3343617) interval=1minute (all duplicates)\n",
      "Fetching 360ONE (3343617) interval=1minute chunk 2024-12-31 12:32:33.772354+00:00 -> 2025-01-07 12:32:33.772354+00:00 (attempt 1)\n",
      "No new rows for 360ONE (3343617) interval=1minute (all duplicates)\n",
      "Fetching 360ONE (3343617) interval=1minute chunk 2025-01-07 12:32:34.772354+00:00 -> 2025-01-14 12:32:34.772354+00:00 (attempt 1)\n",
      "No new rows for 360ONE (3343617) interval=1minute (all duplicates)\n",
      "Fetching 360ONE (3343617) interval=1minute chunk 2025-01-14 12:32:35.772354+00:00 -> 2025-01-21 12:32:35.772354+00:00 (attempt 1)\n",
      "No new rows for 360ONE (3343617) interval=1minute (all duplicates)\n",
      "Fetching 360ONE (3343617) interval=1minute chunk 2025-01-21 12:32:36.772354+00:00 -> 2025-01-28 12:32:36.772354+00:00 (attempt 1)\n",
      "No new rows for 360ONE (3343617) interval=1minute (all duplicates)\n",
      "Fetching 360ONE (3343617) interval=1minute chunk 2025-01-28 12:32:37.772354+00:00 -> 2025-02-04 12:32:37.772354+00:00 (attempt 1)\n",
      "No new rows for 360ONE (3343617) interval=1minute (all duplicates)\n",
      "Fetching 360ONE (3343617) interval=1minute chunk 2025-02-04 12:32:38.772354+00:00 -> 2025-02-11 12:32:38.772354+00:00 (attempt 1)\n",
      "No new rows for 360ONE (3343617) interval=1minute (all duplicates)\n",
      "Fetching 360ONE (3343617) interval=1minute chunk 2025-02-11 12:32:39.772354+00:00 -> 2025-02-18 12:32:39.772354+00:00 (attempt 1)\n",
      "No new rows for 360ONE (3343617) interval=1minute (all duplicates)\n",
      "Fetching 360ONE (3343617) interval=1minute chunk 2025-02-18 12:32:40.772354+00:00 -> 2025-02-25 12:32:40.772354+00:00 (attempt 1)\n",
      "No new rows for 360ONE (3343617) interval=1minute (all duplicates)\n",
      "Fetching 360ONE (3343617) interval=1minute chunk 2025-02-25 12:32:41.772354+00:00 -> 2025-03-04 12:32:41.772354+00:00 (attempt 1)\n",
      "No new rows for 360ONE (3343617) interval=1minute (all duplicates)\n",
      "Fetching 360ONE (3343617) interval=1minute chunk 2025-03-04 12:32:42.772354+00:00 -> 2025-03-11 12:32:42.772354+00:00 (attempt 1)\n",
      "No new rows for 360ONE (3343617) interval=1minute (all duplicates)\n",
      "Fetching 360ONE (3343617) interval=1minute chunk 2025-03-11 12:32:43.772354+00:00 -> 2025-03-18 12:32:43.772354+00:00 (attempt 1)\n",
      "No new rows for 360ONE (3343617) interval=1minute (all duplicates)\n",
      "Fetching 360ONE (3343617) interval=1minute chunk 2025-03-18 12:32:44.772354+00:00 -> 2025-03-25 12:32:44.772354+00:00 (attempt 1)\n",
      "No new rows for 360ONE (3343617) interval=1minute (all duplicates)\n",
      "Fetching 360ONE (3343617) interval=1minute chunk 2025-03-25 12:32:45.772354+00:00 -> 2025-04-01 12:32:45.772354+00:00 (attempt 1)\n",
      "No new rows for 360ONE (3343617) interval=1minute (all duplicates)\n",
      "Fetching 360ONE (3343617) interval=1minute chunk 2025-04-01 12:32:46.772354+00:00 -> 2025-04-08 12:32:46.772354+00:00 (attempt 1)\n",
      "No new rows for 360ONE (3343617) interval=1minute (all duplicates)\n",
      "Fetching 360ONE (3343617) interval=1minute chunk 2025-04-08 12:32:47.772354+00:00 -> 2025-04-15 12:32:47.772354+00:00 (attempt 1)\n",
      "No new rows for 360ONE (3343617) interval=1minute (all duplicates)\n",
      "Fetching 360ONE (3343617) interval=1minute chunk 2025-04-15 12:32:48.772354+00:00 -> 2025-04-22 12:32:48.772354+00:00 (attempt 1)\n",
      "No new rows for 360ONE (3343617) interval=1minute (all duplicates)\n",
      "Fetching 360ONE (3343617) interval=1minute chunk 2025-04-22 12:32:49.772354+00:00 -> 2025-04-29 12:32:49.772354+00:00 (attempt 1)\n",
      "No new rows for 360ONE (3343617) interval=1minute (all duplicates)\n",
      "Fetching 360ONE (3343617) interval=1minute chunk 2025-04-29 12:32:50.772354+00:00 -> 2025-05-06 12:32:50.772354+00:00 (attempt 1)\n",
      "No new rows for 360ONE (3343617) interval=1minute (all duplicates)\n",
      "Fetching 360ONE (3343617) interval=1minute chunk 2025-05-06 12:32:51.772354+00:00 -> 2025-05-13 12:32:51.772354+00:00 (attempt 1)\n",
      "No new rows for 360ONE (3343617) interval=1minute (all duplicates)\n",
      "Fetching 360ONE (3343617) interval=1minute chunk 2025-05-13 12:32:52.772354+00:00 -> 2025-05-20 12:32:52.772354+00:00 (attempt 1)\n",
      "No new rows for 360ONE (3343617) interval=1minute (all duplicates)\n",
      "Fetching 360ONE (3343617) interval=1minute chunk 2025-05-20 12:32:53.772354+00:00 -> 2025-05-27 12:32:53.772354+00:00 (attempt 1)\n",
      "No new rows for 360ONE (3343617) interval=1minute (all duplicates)\n",
      "Fetching 360ONE (3343617) interval=1minute chunk 2025-05-27 12:32:54.772354+00:00 -> 2025-06-03 12:32:54.772354+00:00 (attempt 1)\n",
      "No new rows for 360ONE (3343617) interval=1minute (all duplicates)\n",
      "Fetching 360ONE (3343617) interval=1minute chunk 2025-06-03 12:32:55.772354+00:00 -> 2025-06-10 12:32:55.772354+00:00 (attempt 1)\n",
      "No new rows for 360ONE (3343617) interval=1minute (all duplicates)\n",
      "Fetching 360ONE (3343617) interval=1minute chunk 2025-06-10 12:32:56.772354+00:00 -> 2025-06-17 12:32:56.772354+00:00 (attempt 1)\n",
      "No new rows for 360ONE (3343617) interval=1minute (all duplicates)\n",
      "Fetching 360ONE (3343617) interval=1minute chunk 2025-06-17 12:32:57.772354+00:00 -> 2025-06-24 12:32:57.772354+00:00 (attempt 1)\n",
      "No new rows for 360ONE (3343617) interval=1minute (all duplicates)\n",
      "Fetching 360ONE (3343617) interval=1minute chunk 2025-06-24 12:32:58.772354+00:00 -> 2025-07-01 12:32:58.772354+00:00 (attempt 1)\n",
      "No new rows for 360ONE (3343617) interval=1minute (all duplicates)\n",
      "Fetching 360ONE (3343617) interval=1minute chunk 2025-07-01 12:32:59.772354+00:00 -> 2025-07-08 12:32:59.772354+00:00 (attempt 1)\n",
      "No new rows for 360ONE (3343617) interval=1minute (all duplicates)\n",
      "Fetching 360ONE (3343617) interval=1minute chunk 2025-07-08 12:33:00.772354+00:00 -> 2025-07-15 12:33:00.772354+00:00 (attempt 1)\n",
      "No new rows for 360ONE (3343617) interval=1minute (all duplicates)\n",
      "Fetching 360ONE (3343617) interval=1minute chunk 2025-07-15 12:33:01.772354+00:00 -> 2025-07-22 12:33:01.772354+00:00 (attempt 1)\n",
      "No new rows for 360ONE (3343617) interval=1minute (all duplicates)\n",
      "Fetching 360ONE (3343617) interval=1minute chunk 2025-07-22 12:33:02.772354+00:00 -> 2025-07-29 12:33:02.772354+00:00 (attempt 1)\n",
      "No new rows for 360ONE (3343617) interval=1minute (all duplicates)\n",
      "Fetching 360ONE (3343617) interval=1minute chunk 2025-07-29 12:33:03.772354+00:00 -> 2025-08-05 12:33:03.772354+00:00 (attempt 1)\n",
      "No new rows for 360ONE (3343617) interval=1minute (all duplicates)\n",
      "Fetching 360ONE (3343617) interval=1minute chunk 2025-08-05 12:33:04.772354+00:00 -> 2025-08-12 12:33:04.772354+00:00 (attempt 1)\n",
      "No new rows for 360ONE (3343617) interval=1minute (all duplicates)\n",
      "Fetching 360ONE (3343617) interval=1minute chunk 2025-08-12 12:33:05.772354+00:00 -> 2025-08-19 12:33:05.772354+00:00 (attempt 1)\n",
      "No new rows for 360ONE (3343617) interval=1minute (all duplicates)\n",
      "Fetching 360ONE (3343617) interval=1minute chunk 2025-08-19 12:33:06.772354+00:00 -> 2025-08-26 12:33:06.772354+00:00 (attempt 1)\n",
      "No new rows for 360ONE (3343617) interval=1minute (all duplicates)\n",
      "Fetching 360ONE (3343617) interval=1minute chunk 2025-08-26 12:33:07.772354+00:00 -> 2025-09-02 12:33:07.772354+00:00 (attempt 1)\n",
      "No new rows for 360ONE (3343617) interval=1minute (all duplicates)\n",
      "Fetching 360ONE (3343617) interval=1minute chunk 2025-09-02 12:33:08.772354+00:00 -> 2025-09-09 12:33:08.772354+00:00 (attempt 1)\n",
      "No new rows for 360ONE (3343617) interval=1minute (all duplicates)\n",
      "Fetching 360ONE (3343617) interval=1minute chunk 2025-09-09 12:33:09.772354+00:00 -> 2025-09-16 12:33:09.772354+00:00 (attempt 1)\n",
      "No new rows for 360ONE (3343617) interval=1minute (all duplicates)\n",
      "Fetching 360ONE (3343617) interval=1minute chunk 2025-09-16 12:33:10.772354+00:00 -> 2025-09-23 12:33:10.772354+00:00 (attempt 1)\n",
      "No new rows for 360ONE (3343617) interval=1minute (all duplicates)\n",
      "Fetching 360ONE (3343617) interval=1minute chunk 2025-09-23 12:33:11.772354+00:00 -> 2025-09-30 12:33:11.772354+00:00 (attempt 1)\n",
      "No new rows for 360ONE (3343617) interval=1minute (all duplicates)\n",
      "Fetching 360ONE (3343617) interval=1minute chunk 2025-09-30 12:33:12.772354+00:00 -> 2025-10-07 12:33:12.772354+00:00 (attempt 1)\n",
      "No new rows for 360ONE (3343617) interval=1minute (all duplicates)\n",
      "Fetching 360ONE (3343617) interval=1minute chunk 2025-10-07 12:33:13.772354+00:00 -> 2025-10-14 12:33:13.772354+00:00 (attempt 1)\n",
      "No new rows for 360ONE (3343617) interval=1minute (all duplicates)\n",
      "Fetching 360ONE (3343617) interval=1minute chunk 2025-10-14 12:33:14.772354+00:00 -> 2025-10-21 12:33:14.772354+00:00 (attempt 1)\n",
      "No new rows for 360ONE (3343617) interval=1minute (all duplicates)\n",
      "Fetching 360ONE (3343617) interval=1minute chunk 2025-10-21 12:33:15.772354+00:00 -> 2025-10-28 12:33:15.772354+00:00 (attempt 1)\n",
      "No new rows for 360ONE (3343617) interval=1minute (all duplicates)\n",
      "Fetching 360ONE (3343617) interval=1minute chunk 2025-10-28 12:33:16.772354+00:00 -> 2025-11-04 12:33:16.772354+00:00 (attempt 1)\n",
      "No new rows for 360ONE (3343617) interval=1minute (all duplicates)\n",
      "Fetching 360ONE (3343617) interval=1minute chunk 2025-11-04 12:33:17.772354+00:00 -> 2025-11-11 12:33:17.772354+00:00 (attempt 1)\n",
      "No new rows for 360ONE (3343617) interval=1minute (all duplicates)\n",
      "Fetching 360ONE (3343617) interval=1minute chunk 2025-11-11 12:33:18.772354+00:00 -> 2025-11-18 12:33:18.772354+00:00 (attempt 1)\n",
      "No new rows for 360ONE (3343617) interval=1minute (all duplicates)\n",
      "Fetching 360ONE (3343617) interval=1minute chunk 2025-11-18 12:33:19.772354+00:00 -> 2025-11-25 12:33:19.772354+00:00 (attempt 1)\n",
      "No new rows for 360ONE (3343617) interval=1minute (all duplicates)\n",
      "Fetching 360ONE (3343617) interval=1minute chunk 2025-11-25 12:33:20.772354+00:00 -> 2025-11-26 12:32:28.772354+00:00 (attempt 1)\n",
      "Inserted 14 new rows for 360ONE (3343617) interval=1minute\n",
      "[done] 360ONE 1minute: inserted_total=14, chunks=53\n",
      "  3minute: fetching 2 years of data\n",
      "Fetching 360ONE (3343617) interval=3minute chunk 2023-11-27 12:32:28.772354+00:00 -> 2023-12-07 12:32:28.772354+00:00 (attempt 1)\n",
      "Inserted 4 new rows for 360ONE (3343617) interval=3minute\n",
      "Fetching 360ONE (3343617) interval=3minute chunk 2023-12-07 12:32:29.772354+00:00 -> 2023-12-17 12:32:29.772354+00:00 (attempt 1)\n",
      "Inserted 809 new rows for 360ONE (3343617) interval=3minute\n",
      "Fetching 360ONE (3343617) interval=3minute chunk 2023-12-17 12:32:30.772354+00:00 -> 2023-12-27 12:32:30.772354+00:00 (attempt 1)\n",
      "Inserted 4 new rows for 360ONE (3343617) interval=3minute\n",
      "Fetching 360ONE (3343617) interval=3minute chunk 2023-12-27 12:32:31.772354+00:00 -> 2024-01-06 12:32:31.772354+00:00 (attempt 1)\n",
      "Inserted 934 new rows for 360ONE (3343617) interval=3minute\n",
      "Fetching 360ONE (3343617) interval=3minute chunk 2024-01-06 12:32:32.772354+00:00 -> 2024-01-16 12:32:32.772354+00:00 (attempt 1)\n",
      "Inserted 4 new rows for 360ONE (3343617) interval=3minute\n",
      "Fetching 360ONE (3343617) interval=3minute chunk 2024-01-16 12:32:33.772354+00:00 -> 2024-01-26 12:32:33.772354+00:00 (attempt 1)\n",
      "Inserted 934 new rows for 360ONE (3343617) interval=3minute\n",
      "Fetching 360ONE (3343617) interval=3minute chunk 2024-01-26 12:32:34.772354+00:00 -> 2024-02-05 12:32:34.772354+00:00 (attempt 1)\n",
      "Inserted 4 new rows for 360ONE (3343617) interval=3minute\n",
      "Fetching 360ONE (3343617) interval=3minute chunk 2024-02-05 12:32:35.772354+00:00 -> 2024-02-15 12:32:35.772354+00:00 (attempt 1)\n",
      "Inserted 995 new rows for 360ONE (3343617) interval=3minute\n",
      "Fetching 360ONE (3343617) interval=3minute chunk 2024-02-15 12:32:36.772354+00:00 -> 2024-02-25 12:32:36.772354+00:00 (attempt 1)\n",
      "No new rows for 360ONE (3343617) interval=3minute (all duplicates)\n",
      "Fetching 360ONE (3343617) interval=3minute chunk 2024-02-25 12:32:37.772354+00:00 -> 2024-03-06 12:32:37.772354+00:00 (attempt 1)\n",
      "Inserted 4 new rows for 360ONE (3343617) interval=3minute\n",
      "Fetching 360ONE (3343617) interval=3minute chunk 2024-03-06 12:32:38.772354+00:00 -> 2024-03-16 12:32:38.772354+00:00 (attempt 1)\n",
      "Inserted 809 new rows for 360ONE (3343617) interval=3minute\n",
      "Fetching 360ONE (3343617) interval=3minute chunk 2024-03-16 12:32:39.772354+00:00 -> 2024-03-26 12:32:39.772354+00:00 (attempt 1)\n",
      "Inserted 4 new rows for 360ONE (3343617) interval=3minute\n",
      "Fetching 360ONE (3343617) interval=3minute chunk 2024-03-26 12:32:40.772354+00:00 -> 2024-04-05 12:32:40.772354+00:00 (attempt 1)\n",
      "Inserted 870 new rows for 360ONE (3343617) interval=3minute\n",
      "Fetching 360ONE (3343617) interval=3minute chunk 2024-04-05 12:32:41.772354+00:00 -> 2024-04-15 12:32:41.772354+00:00 (attempt 1)\n",
      "Inserted 4 new rows for 360ONE (3343617) interval=3minute\n",
      "Fetching 360ONE (3343617) interval=3minute chunk 2024-04-15 12:32:42.772354+00:00 -> 2024-04-25 12:32:42.772354+00:00 (attempt 1)\n",
      "Inserted 869 new rows for 360ONE (3343617) interval=3minute\n",
      "Fetching 360ONE (3343617) interval=3minute chunk 2024-04-25 12:32:43.772354+00:00 -> 2024-05-05 12:32:43.772354+00:00 (attempt 1)\n",
      "No new rows for 360ONE (3343617) interval=3minute (all duplicates)\n",
      "Fetching 360ONE (3343617) interval=3minute chunk 2024-05-05 12:32:44.772354+00:00 -> 2024-05-15 12:32:44.772354+00:00 (attempt 1)\n",
      "Inserted 4 new rows for 360ONE (3343617) interval=3minute\n",
      "Fetching 360ONE (3343617) interval=3minute chunk 2024-05-15 12:32:45.772354+00:00 -> 2024-05-25 12:32:45.772354+00:00 (attempt 1)\n",
      "Inserted 844 new rows for 360ONE (3343617) interval=3minute\n",
      "Fetching 360ONE (3343617) interval=3minute chunk 2024-05-25 12:32:46.772354+00:00 -> 2024-06-04 12:32:46.772354+00:00 (attempt 1)\n",
      "Inserted 816 new rows for 360ONE (3343617) interval=3minute\n",
      "Fetching 360ONE (3343617) interval=3minute chunk 2024-06-04 12:32:47.772354+00:00 -> 2024-06-14 12:32:47.772354+00:00 (attempt 1)\n",
      "Inserted 1000 new rows for 360ONE (3343617) interval=3minute\n",
      "Fetching 360ONE (3343617) interval=3minute chunk 2024-06-14 12:32:48.772354+00:00 -> 2024-06-24 12:32:48.772354+00:00 (attempt 1)\n",
      "Inserted 625 new rows for 360ONE (3343617) interval=3minute\n",
      "Fetching 360ONE (3343617) interval=3minute chunk 2024-06-24 12:32:49.772354+00:00 -> 2024-07-04 12:32:49.772354+00:00 (attempt 1)\n",
      "Inserted 1000 new rows for 360ONE (3343617) interval=3minute\n",
      "Fetching 360ONE (3343617) interval=3minute chunk 2024-07-04 12:32:50.772354+00:00 -> 2024-07-14 12:32:50.772354+00:00 (attempt 1)\n",
      "Inserted 809 new rows for 360ONE (3343617) interval=3minute\n",
      "Fetching 360ONE (3343617) interval=3minute chunk 2024-07-14 12:32:51.772354+00:00 -> 2024-07-24 12:32:51.772354+00:00 (attempt 1)\n",
      "Inserted 816 new rows for 360ONE (3343617) interval=3minute\n",
      "Fetching 360ONE (3343617) interval=3minute chunk 2024-07-24 12:32:52.772354+00:00 -> 2024-08-03 12:32:52.772354+00:00 (attempt 1)\n",
      "Inserted 934 new rows for 360ONE (3343617) interval=3minute\n",
      "Fetching 360ONE (3343617) interval=3minute chunk 2024-08-03 12:32:53.772354+00:00 -> 2024-08-13 12:32:53.772354+00:00 (attempt 1)\n",
      "Inserted 816 new rows for 360ONE (3343617) interval=3minute\n",
      "Fetching 360ONE (3343617) interval=3minute chunk 2024-08-13 12:32:54.772354+00:00 -> 2024-08-23 12:32:54.772354+00:00 (attempt 1)\n",
      "Inserted 875 new rows for 360ONE (3343617) interval=3minute\n",
      "Fetching 360ONE (3343617) interval=3minute chunk 2024-08-23 12:32:55.772354+00:00 -> 2024-09-02 12:32:55.772354+00:00 (attempt 1)\n",
      "Inserted 750 new rows for 360ONE (3343617) interval=3minute\n",
      "Fetching 360ONE (3343617) interval=3minute chunk 2024-09-02 12:32:56.772354+00:00 -> 2024-09-12 12:32:56.772354+00:00 (attempt 1)\n",
      "Inserted 1000 new rows for 360ONE (3343617) interval=3minute\n",
      "Fetching 360ONE (3343617) interval=3minute chunk 2024-09-12 12:32:57.772354+00:00 -> 2024-09-22 12:32:57.772354+00:00 (attempt 1)\n",
      "Inserted 809 new rows for 360ONE (3343617) interval=3minute\n",
      "Fetching 360ONE (3343617) interval=3minute chunk 2024-09-22 12:32:58.772354+00:00 -> 2024-10-02 12:32:58.772354+00:00 (attempt 1)\n",
      "Inserted 875 new rows for 360ONE (3343617) interval=3minute\n",
      "Fetching 360ONE (3343617) interval=3minute chunk 2024-10-02 12:32:59.772354+00:00 -> 2024-10-12 12:32:59.772354+00:00 (attempt 1)\n",
      "Inserted 875 new rows for 360ONE (3343617) interval=3minute\n",
      "Fetching 360ONE (3343617) interval=3minute chunk 2024-10-12 12:33:00.772354+00:00 -> 2024-10-22 12:33:00.772354+00:00 (attempt 1)\n",
      "Inserted 816 new rows for 360ONE (3343617) interval=3minute\n",
      "Fetching 360ONE (3343617) interval=3minute chunk 2024-10-22 12:33:01.772354+00:00 -> 2024-11-01 12:33:01.772354+00:00 (attempt 1)\n",
      "Inserted 934 new rows for 360ONE (3343617) interval=3minute\n",
      "Fetching 360ONE (3343617) interval=3minute chunk 2024-11-01 12:33:02.772354+00:00 -> 2024-11-11 12:33:02.772354+00:00 (attempt 1)\n",
      "Inserted 712 new rows for 360ONE (3343617) interval=3minute\n",
      "Fetching 360ONE (3343617) interval=3minute chunk 2024-11-11 12:33:03.772354+00:00 -> 2024-11-21 12:33:03.772354+00:00 (attempt 1)\n",
      "Inserted 750 new rows for 360ONE (3343617) interval=3minute\n",
      "Fetching 360ONE (3343617) interval=3minute chunk 2024-11-21 12:33:04.772354+00:00 -> 2024-12-01 12:33:04.772354+00:00 (attempt 1)\n",
      "Inserted 808 new rows for 360ONE (3343617) interval=3minute\n",
      "Fetching 360ONE (3343617) interval=3minute chunk 2024-12-01 12:33:05.772354+00:00 -> 2024-12-11 12:33:05.772354+00:00 (attempt 1)\n",
      "Inserted 942 new rows for 360ONE (3343617) interval=3minute\n",
      "Fetching 360ONE (3343617) interval=3minute chunk 2024-12-11 12:33:06.772354+00:00 -> 2024-12-21 12:33:06.772354+00:00 (attempt 1)\n",
      "Inserted 933 new rows for 360ONE (3343617) interval=3minute\n",
      "Fetching 360ONE (3343617) interval=3minute chunk 2024-12-21 12:33:07.772354+00:00 -> 2024-12-31 12:33:07.772354+00:00 (attempt 1)\n",
      "Inserted 692 new rows for 360ONE (3343617) interval=3minute\n",
      "Fetching 360ONE (3343617) interval=3minute chunk 2024-12-31 12:33:08.772354+00:00 -> 2025-01-10 12:33:08.772354+00:00 (attempt 1)\n",
      "Inserted 1000 new rows for 360ONE (3343617) interval=3minute\n",
      "Fetching 360ONE (3343617) interval=3minute chunk 2025-01-10 12:33:09.772354+00:00 -> 2025-01-20 12:33:09.772354+00:00 (attempt 1)\n",
      "Inserted 750 new rows for 360ONE (3343617) interval=3minute\n",
      "Fetching 360ONE (3343617) interval=3minute chunk 2025-01-20 12:33:10.772354+00:00 -> 2025-01-30 12:33:10.772354+00:00 (attempt 1)\n",
      "Inserted 1000 new rows for 360ONE (3343617) interval=3minute\n",
      "Fetching 360ONE (3343617) interval=3minute chunk 2025-01-30 12:33:11.772354+00:00 -> 2025-02-09 12:33:11.772354+00:00 (attempt 1)\n",
      "Inserted 933 new rows for 360ONE (3343617) interval=3minute\n",
      "Fetching 360ONE (3343617) interval=3minute chunk 2025-02-09 12:33:12.772354+00:00 -> 2025-02-19 12:33:12.772354+00:00 (attempt 1)\n",
      "Inserted 942 new rows for 360ONE (3343617) interval=3minute\n",
      "Fetching 360ONE (3343617) interval=3minute chunk 2025-02-19 12:33:13.772354+00:00 -> 2025-03-01 12:33:13.772354+00:00 (attempt 1)\n",
      "Inserted 808 new rows for 360ONE (3343617) interval=3minute\n",
      "Fetching 360ONE (3343617) interval=3minute chunk 2025-03-01 12:33:14.772354+00:00 -> 2025-03-11 12:33:14.772354+00:00 (attempt 1)\n",
      "Inserted 817 new rows for 360ONE (3343617) interval=3minute\n",
      "Fetching 360ONE (3343617) interval=3minute chunk 2025-03-11 12:33:15.772354+00:00 -> 2025-03-21 12:33:15.772354+00:00 (attempt 1)\n",
      "Inserted 875 new rows for 360ONE (3343617) interval=3minute\n",
      "Fetching 360ONE (3343617) interval=3minute chunk 2025-03-21 12:33:16.772354+00:00 -> 2025-03-31 12:33:16.772354+00:00 (attempt 1)\n",
      "Inserted 683 new rows for 360ONE (3343617) interval=3minute\n",
      "Fetching 360ONE (3343617) interval=3minute chunk 2025-03-31 12:33:17.772354+00:00 -> 2025-04-10 12:33:17.772354+00:00 (attempt 1)\n",
      "Inserted 875 new rows for 360ONE (3343617) interval=3minute\n",
      "Fetching 360ONE (3343617) interval=3minute chunk 2025-04-10 12:33:18.772354+00:00 -> 2025-04-20 12:33:18.772354+00:00 (attempt 1)\n",
      "Inserted 500 new rows for 360ONE (3343617) interval=3minute\n",
      "Fetching 360ONE (3343617) interval=3minute chunk 2025-04-20 12:33:19.772354+00:00 -> 2025-04-30 12:33:19.772354+00:00 (attempt 1)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[96]\u001b[39m\u001b[32m, line 47\u001b[39m\n\u001b[32m     43\u001b[39m start_date = END_DATE - timedelta(days=\u001b[32m365\u001b[39m*lookback_years)\n\u001b[32m     45\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitv\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: fetching \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlookback_years\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m years of data\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m res = \u001b[43mprocess_instrument_interval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43minstrument_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtkn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtradingsymbol\u001b[49m\u001b[43m=\u001b[49m\u001b[43msym\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mitv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart_ts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43mend_ts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEND_DATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunk_days\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCHUNK_DAYS_BY_INTERVAL\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m90\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43msleep_between_chunks\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\n\u001b[32m     56\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m summary.append((sym, itv, lookback_years, res[\u001b[33m'\u001b[39m\u001b[33mstatus\u001b[39m\u001b[33m'\u001b[39m], res.get(\u001b[33m'\u001b[39m\u001b[33minserted\u001b[39m\u001b[33m'\u001b[39m,\u001b[32m0\u001b[39m), res.get(\u001b[33m'\u001b[39m\u001b[33mchunks\u001b[39m\u001b[33m'\u001b[39m,\u001b[32m0\u001b[39m)))\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# sleep between intervals to avoid rate limits\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[95]\u001b[39m\u001b[32m, line 66\u001b[39m, in \u001b[36mprocess_instrument_interval\u001b[39m\u001b[34m(instrument_token, tradingsymbol, interval, start_ts, end_ts, chunk_days, max_retries, sleep_between_chunks)\u001b[39m\n\u001b[32m     63\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m  chunk returned empty.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     65\u001b[39m     \u001b[38;5;66;03m# ingest\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     inserted = \u001b[43mingest_candles_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstrument_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtradingsymbol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexchange\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mNSE\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterval\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m     inserted_total += inserted\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# update last_ingested_ts to chunk_end\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[91]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mingest_candles_df\u001b[39m\u001b[34m(df_candles, instrument_token, tradingsymbol, exchange, interval)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# reorder to match COPY target (excluding auto-generated id and raw columns)\u001b[39;00m\n\u001b[32m     35\u001b[39m df_to_copy = df[[\u001b[33m'\u001b[39m\u001b[33minstrument_token\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mtradingsymbol\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mexchange\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33minterval\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mts\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mopen\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mhigh\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mlow\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mclose\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mvolume\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33moi\u001b[39m\u001b[33m'\u001b[39m]]\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m conn = \u001b[43mpsycopg2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDB_URI\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m cur = conn.cursor()\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     40\u001b[39m     \u001b[38;5;66;03m# Create temp table with same structure\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/BudhiCapital/.venv/lib/python3.12/site-packages/psycopg2/__init__.py:122\u001b[39m, in \u001b[36mconnect\u001b[39m\u001b[34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     kwasync[\u001b[33m'\u001b[39m\u001b[33masync_\u001b[39m\u001b[33m'\u001b[39m] = kwargs.pop(\u001b[33m'\u001b[39m\u001b[33masync_\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    121\u001b[39m dsn = _ext.make_dsn(dsn, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m conn = \u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_factory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconnection_factory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwasync\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cursor_factory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    124\u001b[39m     conn.cursor_factory = cursor_factory\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# CELL X5: orchestrator - safe test run for N instruments and all desired intervals\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# configure what to process\n",
    "LIMIT_INSTRUMENTS = 10   # change after testing\n",
    "END_DATE = datetime.now(timezone.utc)\n",
    "\n",
    "# Define different lookback periods for each interval\n",
    "INTERVAL_LOOKBACK_YEARS = {\n",
    "    '1minute': 1,      # 1 year of 1-minute data\n",
    "    '3minute': 2,      # 2 years of 3-minute data\n",
    "    '5minute': 2,      # 2 years of 5-minute data\n",
    "    '15minute': 3,     # 3 years of 15-minute data\n",
    "    '30minute': 3,     # 3 years of 30-minute data\n",
    "    '60minute': 7,     # 7 years of 60-minute data\n",
    "    '180minute': 7,    # 7 years of 3-hour data\n",
    "    '1day': 20,        # max available daily data\n",
    "    '1week': 20,       # max available weekly data\n",
    "    '1month': 20,      # max available monthly data\n",
    "    '1year': 20,       # max available yearly data\n",
    "}\n",
    "\n",
    "# read candidate instruments (NSE equities)\n",
    "with engine.connect() as conn:\n",
    "    df_candidates = pd.read_sql(\"\"\"\n",
    "      SELECT instrument_token, tradingsymbol, exchange\n",
    "      FROM instruments\n",
    "      WHERE exchange ILIKE 'NSE' AND (instrument_type ILIKE 'EQ' OR instrument_type = '')\n",
    "      ORDER BY tradingsymbol\n",
    "      LIMIT %s\n",
    "    \"\"\", conn, params=(LIMIT_INSTRUMENTS,))\n",
    "\n",
    "print(\"Found instruments:\", len(df_candidates))\n",
    "summary = []\n",
    "for _, r in df_candidates.iterrows():\n",
    "    tkn = int(r['instrument_token'])\n",
    "    sym = r['tradingsymbol']\n",
    "    exch = r['exchange'] or 'NSE'\n",
    "    print(\"=== Processing instrument:\", sym, tkn)\n",
    "    for itv in INTERVALS:\n",
    "        # Get lookback period for this interval\n",
    "        lookback_years = INTERVAL_LOOKBACK_YEARS.get(itv, 3)\n",
    "        start_date = END_DATE - timedelta(days=365*lookback_years)\n",
    "        \n",
    "        print(f\"  {itv}: fetching {lookback_years} years of data\")\n",
    "        \n",
    "        res = process_instrument_interval(\n",
    "            instrument_token=tkn,\n",
    "            tradingsymbol=sym,\n",
    "            interval=itv,\n",
    "            start_ts=start_date,\n",
    "            end_ts=END_DATE,\n",
    "            chunk_days=CHUNK_DAYS_BY_INTERVAL.get(itv, 90),\n",
    "            max_retries=5,\n",
    "            sleep_between_chunks=1.0\n",
    "        )\n",
    "        summary.append((sym, itv, lookback_years, res['status'], res.get('inserted',0), res.get('chunks',0)))\n",
    "        # sleep between intervals to avoid rate limits\n",
    "        time.sleep(2.0)\n",
    "\n",
    "print(\"\\nOrchestrator finished. Summary:\")\n",
    "print(f\"{'Symbol':<12} {'Interval':<12} {'Years':<8} {'Status':<10} {'Inserted':<10} {'Chunks':<8}\")\n",
    "print(\"-\" * 70)\n",
    "for row in summary:\n",
    "    print(f\"{row[0]:<12} {row[1]:<12} {row[2]:<8} {row[3]:<10} {row[4]:<10} {row[5]:<8}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
